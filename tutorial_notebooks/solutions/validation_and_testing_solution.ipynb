{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "lLy5PsIql9A5"
   },
   "source": [
    "# 5. Validation & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6L-ynOvokKT"
   },
   "source": [
    "Welcome to the fifth notebook of our six part series part of our tutorial on Deep Learning for Human Activity Recognition. Within the last notebook you learned:\n",
    "\n",
    "- How do I define a sample neural network architecture in PyTorch? \n",
    "- What additional preprocessing do I need to apply to my data to fed it into my network?\n",
    "- How do I define a train loop which trains my neural network?\n",
    "\n",
    "This notebook will teach you everything you need to know about validation and testing. When building a predictive pipeline there are a lot of parameters which one needs to set before comencing the actual training. Coming up with a suitable set of hyperparameters is called hypertuning. In order to gain feedback whether the applied hyperparameters are a good choice, we check the predictive performance of our model on the validation set. This is called validation.\n",
    "\n",
    "Now you might ask yourself: Solely relying and tuning based on the validation scores would inherit that your trained model would end up being too well optimized on the validation set and thus not general anymore, right? If asked yourself that question, then you are 100% right in your assumption! This is what we call overfitting and is one of the major pitfalls in Machine Learning.Overfitting your model results in bad prediction performance on unseen data. \n",
    "\n",
    "We therefore need a third dataset, called the test dataset. The test dataset is a part of the initial dataset which you keep separate from all optimization steps. It is only used to gain insights on the predictive performance of the model and must not (!) be used as a reference for tuning hyperparameters. As we mentioned in during the theoretical parts of this tutorial, (supervised) Deep Learning, in our opinion, is just a fancy word for function approximation. If your model performs both well during validation and testing, it is a general function which properly approximates the underlying function.\n",
    "\n",
    "After completing this notebook you will be answer the following questions:\n",
    "- How do I split my initial dataset into a train, validation and test dataset?\n",
    "- What validation methods exist in Human Activity Recognition? How are they performed?\n",
    "- How is testing usually performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4fWjW5V0_MT"
   },
   "source": [
    "## 5.1. Important Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkhCF6Pd1B1Z"
   },
   "source": [
    "If you are accessing this tutorial via [Google Colab](https://colab.research.google.com/github/mariusbock/dl-for-har/blob/main/tutorial_notebooks/training.ipynb), first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
    "1. On Google Colab, go to `Help` -> `View in English` \n",
    "2. Change the default language of your browser to `English`.\n",
    "\n",
    "To also ease the communication when communicating errors, enable line numbers within the settings of Colab.\n",
    "\n",
    "1. On Google Colab, go to `Tools` -> `Settings` -> `Editor` -> `Show line numbers`\n",
    "\n",
    "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
    "\n",
    "- `Runtime` -> `Change runtime type` -> `Dropdown` -> `GPU` -> `Save`\n",
    "\n",
    "**Hint:** you can auto-complete code in Colab via `ctrl` + `spacebar`\n",
    "\n",
    "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our [GitHub repository](https://github.com/mariusbock/dl-for-har).\n",
    "\n",
    "To get started with this notebook, you need to first run the code cell below. Please set `use_colab` to be `True` if you are accessing this notebook via Colab. If not, please set it to `False`. This code cell will make sure that imports from our GitHub repository will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "si3n5Sc51L-D"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "use_colab = True\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if use_colab:\n",
    "    # move to content directory and remove directory for a clean start \n",
    "    %cd /content/         \n",
    "    %rm -rf dl-for-har\n",
    "    # clone package repository (will throw error if already cloned)\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/       \n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "    \n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIjrK-KE1iDL"
   },
   "source": [
    "## 5.1. Splitting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrLU2e9H1oAX"
   },
   "source": [
    "Within the first part of this notebook we will split our data in the above mentioned three datasets, namely the train, validation and test dataset. There are multiple ways how to split the data into the two respective datasets, for example:\n",
    "\n",
    "- **Subject-wise:** split according to participants within the dataset. This means that we are reserving certain subjects to be included in the train, validation and test set respectively. For example, given that there are a total of 10 subjects, you could use 6 subjects for trainig, 2 subjects for validation and 2 subjects for testing.\n",
    "- **Percentage-wise:** state how large percentage-wise your train, validation and test dataset should be compared to the full dataset. For example, you could use 60% of your data for training, 20% for validation and 20% for testing. The three splits can also be chosen to be stratified, meaning that the relative label distribution within each of the two dataset is kept the same as in the full dataset. Note that stratifiying your data would require the data to be shuffled.\n",
    "- **Record-wise:** state how many records should be in your train, validation and test dataset should be contained, i.e. define two cutoff points. For example, given that there are 1 million records in your full dataset, you could have the first 600 thousand records to be contained in the train dataset, the next 200 thousand in the validation dataset and the remaining 200 thousand records to be contained in the test dataset.\n",
    "\n",
    "**WARNING:** shuffling your dataset during splitting (which is e.g. needed for stratified splits) will destroy the time-dependencies among the data records. To minimize this effect, apply a sliding window on top of your data before splitting. This way, time-dependencies will at least be preserved within the windows. While working on this notebook, we will notify you when this is necessary.\n",
    "\n",
    "To keep things simple and fast, we will be splitting our data subject-wise. We will use the first data of the first subject for training, the data of the second subject for validation and the data of the third subject for testing. Your first task will be to perform said split. Note that we already imported the dataset for you using the `load_dataset()` function, which is part of the DL-ARC feature stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIAoSI0Ql9BC"
   },
   "source": [
    "### Task 1: Split the data into train, validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_QkR_bHl9BC"
   },
   "source": [
    "1. Define the `train` dataset to be the data of the first subject, i.e. with `subject_identifier = 0`. (`lines 13-14`)\n",
    "2. Define the `valid` dataset to be the data of the second subject, i.e. with `subject_identifier = 1`. (`lines 15-16`)\n",
    "3. Define the `test` dataset to be the data of the third subject, i.e. with `subject_identifier = 2`. (`lines 17-18`)\n",
    "4. Define a fourth dataset being a concatenated version of the `train` and `valid` dataset called `train_valid`. You will need this dataset for some of the validation methods. Use `np.concatenate()` in order to concat the two numpy arrays along `axis=0`. (`lines 20-21`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el2x8KMJl9BE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data_processing.preprocess_data import load_dataset\n",
    "\n",
    "\n",
    "# data loading (we are using a predefined method called load_dataset, which is part of the DL-ARC feature stack)\n",
    "X, y, num_classes, class_names, sampling_rate, has_null = load_dataset('rwhar_3sbjs', include_null=True)\n",
    "# since the method returns features and labels separatley, we need to concat them\n",
    "data = np.concatenate((X, y[:, None]), axis=1)\n",
    "\n",
    "# define the train data to be the data of the first subject\n",
    "train_data = data[data[:, 0] == 0]\n",
    "# define the valid data to be the data of the second subject\n",
    "valid_data = data[data[:, 0] == 1]\n",
    "# define the test data to be the data of the third subject\n",
    "test_data = data[data[:, 0] == 2]\n",
    "\n",
    "# define the train_valid_data by concatenating the train and validation dataset \n",
    "train_valid_data = np.concatenate((train_data, valid_data), axis=0)\n",
    "\n",
    "print('\\nShape of the train, validation and test dataset:')\n",
    "print(train_data.shape, valid_data.shape, test_data.shape)\n",
    "print('\\nShape of the concatenated train_valid dataset:')\n",
    "print(train_valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzCPsMcd4koA"
   },
   "source": [
    "## 5.2. Define the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_q8TpPal9BE"
   },
   "source": [
    "Before we go over talking about how to perform validation in Human Activtiy Recognition, we need to define our hyperparameters again. As you know from the previous notebook, it is common practice to track all your settings and parameters in a compiled `config` object. Due to fact that we will be using pre-implemented methods of the feature stack of the DL-ARC GitHub, we will now need to define a more complex `config` object. \n",
    "\n",
    "Within the next code block we defined a sample `config` object for you. It contains some parameters which you already know from previous notebooks, but also lots which you don't know. We will not cover all of them during this tutorial, but encourage you to check out the complete implementation of the DL-ARC. We also separated the parameters into two groups for you, once which you can play around with and ones which you should handle with care and rather leave as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jjZYXFX6l9BF"
   },
   "outputs": [],
   "source": [
    "from misc.torchutils import seed_torch\n",
    "\n",
    "config = {\n",
    "    #### TRY AND CHANGE THESE PARAMETERS ####\n",
    "    # sliding window settings\n",
    "    'sw_length': 50,\n",
    "    'sw_unit': 'units',\n",
    "    'sampling_rate': 50,\n",
    "    'sw_overlap': 30,\n",
    "    # network settings\n",
    "    'nb_conv_blocks': 2,\n",
    "    'conv_block_type': 'normal',\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 11,\n",
    "    'nb_units_lstm': 128,\n",
    "    'nb_layers_lstm': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    # training settings\n",
    "    'epochs': 10,\n",
    "    'batch_size': 100,\n",
    "    'loss': 'cross_entropy',\n",
    "    'weighted': True,\n",
    "    'weights_init': 'xavier_uniform',\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-6,\n",
    "    'shuffling': True,\n",
    "    'no_lstm': False,\n",
    "    ### UP FROM HERE YOU SHOULD RATHER NOT CHANGE THESE ####\n",
    "    'batch_norm': False,\n",
    "    'dilation': 1,\n",
    "    'pooling': False,\n",
    "    'pool_type': 'max',\n",
    "    'pool_kernel_width': 2,\n",
    "    'reduce_layer': False,\n",
    "    'reduce_layer_output': 10,\n",
    "    'nb_classes': 8,\n",
    "    'seed': 1,\n",
    "    'gpu': 'cuda:0',\n",
    "    'verbose': False,\n",
    "    'print_freq': 10,\n",
    "    'save_gradient_plot': False,\n",
    "    'print_counts': False,\n",
    "    'adj_lr': False,\n",
    "    'adj_lr_patience': 5,\n",
    "    'early_stopping': False,\n",
    "    'es_patience': 5,\n",
    "    'save_test_preds': False,\n",
    "    'valid_epoch': 'last'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0WRQj1dl9BF"
   },
   "source": [
    "## 5.3. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhwdRETQ8t9D"
   },
   "source": [
    "Within the next segment we will explain the most prominent validation methods used in Human Activity Recognition. These are:\n",
    "\n",
    "- Train-Valid Split\n",
    "- k-Fold Cross-Validation\n",
    "- Cross-Participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQPUaRRE8AC9"
   },
   "source": [
    "### 5.3.1. Train-Valid Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8BS4Qf3l9BF"
   },
   "source": [
    "The train-valid split is one of the most basic validation method, which you  already did yourself. Instead of varying the validation set and getting a more holistic view, we define it to be a set part of the data. As mentioned above there are multiple ways how to do so. For simplicity purposes, we chose to use a subject-wise split. Within the next task you will be asked to train your network using the `train` data and obtain predictions on the `valid` data. We do not ask you to define the training loop again and allow you to use the built-in `train` function of the DL-ARC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOcMYLvTl9BF"
   },
   "source": [
    "#### Task 2: Implementing the train-valid split validation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsRp7hDNl9BF"
   },
   "source": [
    "1. As you already defined the train and valid dataset you can go ahead and apply a sliding window on top of both datasets. You can use the predefined method `apply_sliding_window()`, which is part of the DL-ARC pipeline, to do so. It is already be imported for you. We will give you hints on what to pass the method. (`lines 22-30`)\n",
    "2. (*Optional*) Omit the first feature column (subject_identifier) from the train and validation dataset. (`lines 32-34`)\n",
    "3. Within the `config` object, set the parameters `window_size` and `nb_channels` accordingly. (`lines 36-40`)\n",
    "4. Define the `DeepConvLSTM` object. It is already imported for you. Also define the `optimizer` being the [Adam optimizer](https://pytorch.org/docs/stable/optim.html) and `criterion` being the [Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (`lines 42-48`)\n",
    "5. Convert the feature columns of the train and validation to `float32` and label column to `uint8` for GPU compatibility. Use the [built-in function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) of a pandas dataframe called `astype()`. (`lines 50-52`)\n",
    "6. Use both datasets to run the `train()` function. (`lines 54-55`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61uZSoSdl9BG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from model.train import train\n",
    "from model.DeepConvLSTM import DeepConvLSTM\n",
    "from data_processing.sliding_window import apply_sliding_window\n",
    "from misc.torchutils import seed_torch\n",
    "\n",
    "\n",
    "# in order to get reproducible results, we need to seed torch and other random parts of our implementation\n",
    "seed_torch(config['seed'])\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "print(train_data.shape, valid_data.shape)\n",
    "\n",
    "# apply the sliding window on top of both the train and validation data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_train, y_train = apply_sliding_window(train_data[:, :-1], train_data[:, -1], sliding_window_size=config['sw_length'], unit=config['sw_unit'], sampling_rate=config['sampling_rate'], sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_valid, y_valid = apply_sliding_window(valid_data[:, :-1], valid_data[:, -1], sliding_window_size=config['sw_length'], unit=config['sw_unit'], sampling_rate=config['sampling_rate'], sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "# you can do it if you want to as it is not a useful feature\n",
    "X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "\n",
    "# within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "# window_size = size of the sliding window in units\n",
    "# nb_channels = number of feature channels\n",
    "config['window_size'] = X_train.shape[1]\n",
    "config['nb_channels'] = X_train.shape[2]\n",
    "\n",
    "# define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "# pass it the config object\n",
    "net = DeepConvLSTM(config=config)\n",
    "\n",
    "# defines the loss and optimizer\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "# convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "\n",
    "# feed the datasets into the train function; can be imported from model.train\n",
    "train_valid_net, _, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, optimizer=opt, loss=loss, config=config, log_date=log_date, log_timestamp=log_timestamp)\n",
    "\n",
    "# the next bit prints out your results if you did everything correctly\n",
    "cls = np.array(range(config['nb_classes']))\n",
    "\n",
    "print('\\nVALIDATION RESULTS: ')\n",
    "print(\"\\nAvg. Accuracy: {0}\".format(jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. Precision: {0}\".format(precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. Recall: {0}\".format(recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. F1: {0}\".format(f1_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "\n",
    "print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "print(\"\\nAccuracy:\")\n",
    "for i, rslt in enumerate(jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nPrecision:\")\n",
    "for i, rslt in enumerate(precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nRecall:\")\n",
    "for i, rslt in enumerate(recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nF1:\")\n",
    "for i, rslt in enumerate(f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "\n",
    "print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "print(\"\\nTrain-Val-Accuracy Difference: {0}\".format(jaccard_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                  jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-Precision Difference: {0}\".format(precision_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                   precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-Recall Difference: {0}\".format(recall_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-F1 Difference: {0}\".format(f1_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                            f1_score(val_output[:, 1], val_output[:, 0], average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BepnQk8l9BH"
   },
   "source": [
    "### 5.3.2. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OlOGi55l9BJ"
   },
   "source": [
    "The k-fold cross-validation is the most popular form of cross-validation. Instead of only splitting our data once into a train and validation dataset, like we did in the previous validation method, we take the average of k different train-valid splits. To do so we take our concatenated version of the train and validation set and split it into k equal-sized chunks of data. A so-called fold is now that we train our network using all but one of these chunks of data and validate it using the chunk we excluded (thus being unseen data). The process is repeated k-times, i.e. k-folds, so that each chunk of data is the validation dataset exactly once. Note that with each fold, the network needs to be reinitialized, i.e. trained from scratch, to ensure that it is not predicting already seen data.\n",
    "\n",
    "\n",
    "**Note:** It is recommended to use stratified k-fold cross-validation, i.e. each of the k chunks of data has the same distribution of labels as the original full dataset. This avoids the risk, especially for unbalanced datasets, of having certain labels missing within the train dataset, which would cause the validation process to break. Nevertheless, as also stated above, stratification requires shuffeling and thus one should always first apply the sliding window before applying the split.\n",
    "\n",
    "The next task will lead you through the implementation of the k-fold cross-validation loop. In order to chunk your data and also apply stratification, we recommend you to use the scikit-learn helper object for stratified k-fold cross-validation called `StratifiedKFold`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sK2SCNYl9BJ"
   },
   "source": [
    "#### Task 3: Implementing the k-fold CV loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iirq5yFll9BK"
   },
   "source": [
    "1. Define the scikit-learn helper object for stratified k-fold cross-validation called `StratifiedKFold`. It is already imported for you. We will also give you hints what to pass it as arguments. (`lines 14-16`)\n",
    "2. Apply the `apply_sliding_window()` function on top of the `train_valid_data` object which you previously defined. (`lines 20-24`)\n",
    "3. (*Optional*) Omit the first feature column (subject_identifier) from the `train_valid_data` dataset. (`lines 26-28`)\n",
    "4. Define the k-fold loop; use the `split()` function of the `StratifiedKFold` object to obtain indeces to split the `train_valid_data` (`lines 42-49`)\n",
    "5. Within the `config` object, set the parameters `window_size` and `nb_channels` accordingly. (`lines 51-55`)\n",
    "6. Define the `DeepConvLSTM` object. It is already imported for you. Also define the `optimizer` being the [Adam optimizer](https://pytorch.org/docs/stable/optim.html) and `criterion` being the [Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (`lines 57-63`)\n",
    "7. Convert the feature columns of the train and validation to `float32` and label column to `uint8` for GPU compatibility. Use the [built-in function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) of a pandas dataframe called `astype()`. (`lines 65-67`)\n",
    "8. Use both datasets to run the `train()` function. (`lines 69-70`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wnh-tBGAl9BK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# number of splits, i.e. folds\n",
    "config['splits_kfold'] = 10\n",
    "\n",
    "# in order to get reproducible results, we need to seed torch and other random parts of our implementation\n",
    "seed_torch(config['seed'])\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# define the stratified k-fold object; it is already imported for you\n",
    "# pass it the number of splits, i.e. folds, and seed as well as set shuffling to true\n",
    "skf = StratifiedKFold(n_splits=config['splits_kfold'], random_state=config['seed'])\n",
    "    \n",
    "print(train_valid_data.shape)\n",
    "\n",
    "# apply the sliding window on top of both the train_valid_data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_train_valid, y_train_valid = apply_sliding_window(train_valid_data[:, :-1], train_valid_data[:, -1], sliding_window_size=config['sw_length'], unit=config['sw_unit'], sampling_rate=config['sampling_rate'], sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "print(X_train_valid.shape, y_train_valid.shape)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the train _valid_data\n",
    "# you can do it if you want to as it is not a useful feature\n",
    "X_train_valid = X_train_valid[:, :, 1:]\n",
    "\n",
    "# result objects used for accumulating the scores across folds; add each fold result to these objects so that they\n",
    "# are averaged at the end of the k-fold loop\n",
    "kfold_accuracy = np.zeros(config['nb_classes'])\n",
    "kfold_precision = np.zeros(config['nb_classes'])\n",
    "kfold_recall = np.zeros(config['nb_classes'])\n",
    "kfold_f1 = np.zeros(config['nb_classes'])\n",
    "    \n",
    "kfold_accuracy_gap = 0\n",
    "kfold_precision_gap = 0\n",
    "kfold_recall_gap = 0\n",
    "kfold_f1_gap = 0\n",
    "\n",
    "# k-fold validation loop; for each loop iteration return fold identifier and indeces which can be used to split\n",
    "# the train + valid data into train and validation data according to the current fold\n",
    "for j, (train_index, valid_index) in enumerate(skf.split(X_train_valid, y_train_valid)):\n",
    "    print('\\nFold {0}/{1}'.format(j + 1, config['splits_kfold']))\n",
    "    \n",
    "    # split the data into train and validation data; to do so, use the indeces produces by the split function\n",
    "    X_train, X_valid = X_train_valid[train_index], X_train_valid[valid_index]\n",
    "    y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "    \n",
    "    # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "    # window_size = size of the sliding window in units\n",
    "    # nb_channels = number of feature channels\n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    \n",
    "    # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "    # pass it the config object\n",
    "    net = DeepConvLSTM(config=config)\n",
    "    \n",
    "    # defines the loss and optimizer\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    # convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "    X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "    X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "    \n",
    "    # feed the datasets into the train function; can be imported from model.train\n",
    "    kfold_net, _, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, optimizer=opt, loss=loss, config=config, log_date=log_date, log_timestamp=log_timestamp)\n",
    "        \n",
    "    # in the following validation and train evaluation metrics are calculated\n",
    "    cls = np.array(range(config['nb_classes']))\n",
    "    val_accuracy = jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_precision = precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_recall = recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_f1 = f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    train_accuracy = jaccard_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_precision = precision_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_recall = recall_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_f1 = f1_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    \n",
    "    # add up the fold results\n",
    "    kfold_accuracy += val_accuracy\n",
    "    kfold_precision += val_precision\n",
    "    kfold_recall += val_recall\n",
    "    kfold_f1 += val_f1\n",
    "\n",
    "    # add up the generalization gap results\n",
    "    kfold_accuracy_gap += train_accuracy - val_accuracy\n",
    "    kfold_precision_gap += train_precision - val_precision\n",
    "    kfold_recall_gap += train_recall - val_recall\n",
    "    kfold_f1_gap += train_f1 - val_f1\n",
    "    \n",
    "# the next bit prints out the average results across folds if you did everything correctly\n",
    "print(\"\\nK-FOLD VALIDATION RESULTS: \")\n",
    "print(\"Accuracy: {0}\".format(np.mean(kfold_accuracy / config['splits_kfold'])))\n",
    "print(\"Precision: {0}\".format(np.mean(kfold_precision / config['splits_kfold'])))\n",
    "print(\"Recall: {0}\".format(np.mean(kfold_recall / config['splits_kfold'])))\n",
    "print(\"F1: {0}\".format(np.mean(kfold_f1 / config['splits_kfold'])))\n",
    "    \n",
    "print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "print(\"\\nAccuracy:\")\n",
    "for i, rslt in enumerate(kfold_accuracy / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nPrecision:\")\n",
    "for i, rslt in enumerate(kfold_precision / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nRecall:\")\n",
    "for i, rslt in enumerate(kfold_recall / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nF1:\")\n",
    "for i, rslt in enumerate(kfold_f1 / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    \n",
    "print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "print(\"\\nAccuracy: {0}\".format(kfold_accuracy_gap / config['splits_kfold']))\n",
    "print(\"Precision: {0}\".format(kfold_precision_gap / config['splits_kfold']))\n",
    "print(\"Recall: {0}\".format(kfold_recall_gap / config['splits_kfold']))\n",
    "print(\"F1: {0}\".format(kfold_f1_gap / config['splits_kfold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI5ztrFyl9BL"
   },
   "source": [
    "### 5.3.3. Cross-Participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgc5fBYMl9BL"
   },
   "source": [
    "Cross-participant cross-validation, also known as Leave-One-Subject-Out (LOSO) cross-validation is the most complex, but also most expressive validation method one can apply when dealing with multi-subject data. In general, it can be seen as a variation of the k-fold cross-validation with k being the number of subjects. Within each fold, you train your network on the data of all but one subject and validate it on the left-out subject. The process is repeated as many times as there are subjects so that each subject becomes the validation set exaclty once. This way, each subject is treated as the unseen data at least once. \n",
    "\n",
    "Leaving one subject out each fold ensures that the overall evaluation of the algorithm does not overfit on subject-specific traits, i.e. how subjects performed the activities individually. It is therefore a great method to obtain a model which is good at predicting activities no matter which person performs them, i.e. a more general model!\n",
    "\n",
    "The next task will lead you through the implementation of the cross-participant cross-validation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxmMLN71l9BM"
   },
   "source": [
    "#### Task 4: Implementing the cross-participant CV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yaGaXanl9BM"
   },
   "source": [
    "1. Define a loop which iterates over the identifiers of all subjects. (`lines 8-10`)\n",
    "2. Define the `train` data to be everything but the current subject's data and the `valid` data to be the current subject's data by filtering the `train_valid_data`. (`lines 12-15`)\n",
    "3. Apply the `apply_sliding_window()` function on top of the filtered datasets you just defined. (`lines 19-27`)\n",
    "4. (*Optional*) Omit the first feature column (subject_identifier) from the train and validation dataset. (`lines 29-31`)\n",
    "5. Within the `config` object, set the parameters `window_size` and `nb_channels` accordingly. (`lines 51-55`)\n",
    "6. Define the `DeepConvLSTM` object. It is already imported for you. Also define the `optimizer` being the [Adam optimizer](https://pytorch.org/docs/stable/optim.html) and `criterion` being the [Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (`lines 39-45`)\n",
    "7. Convert the feature columns of the train and validation to `float32` and label column to `uint8` for GPU compatibility. Use the [built-in function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) of a pandas dataframe called `astype()`. (`lines 47-49`)\n",
    "8. Use both datasets to run the `train()` function. (`lines 51-52`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcUekkJal9BM"
   },
   "outputs": [],
   "source": [
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# in order to get reproducible results, we need to seed torch and other random parts of our implementation\n",
    "seed_torch(config['seed'])\n",
    "\n",
    "# iterate over all subjects\n",
    "for i, sbj in enumerate(np.unique(train_valid_data[:, 0])):\n",
    "    print('\\n VALIDATING FOR SUBJECT {0} OF {1}'.format(int(sbj) + 1, int(np.max(train_valid_data[:, 0])) + 1))\n",
    "    \n",
    "    # define the train data to be everything, but the data of the current subject\n",
    "    train_data = train_valid_data[train_valid_data[:, 0] != sbj]\n",
    "    # define the validation data to be the data of the current subject\n",
    "    valid_data = train_valid_data[train_valid_data[:, 0] == sbj]\n",
    "    \n",
    "    print(train_data.shape, valid_data.shape)\n",
    "    \n",
    "    # apply the sliding window on top of both the train and validation data; use the \"apply_sliding_window\" function\n",
    "    # found in data_processing.sliding_window\n",
    "    X_train, y_train = apply_sliding_window(train_data[:, :-1], train_data[:, -1], sliding_window_size=config['sw_length'], unit=config['sw_unit'], sampling_rate=config['sampling_rate'], sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    X_valid, y_valid = apply_sliding_window(valid_data[:, :-1], valid_data[:, -1], sliding_window_size=config['sw_length'], unit=config['sw_unit'], sampling_rate=config['sampling_rate'], sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "    print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "    # (optional) omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "    # you can do it if you want to as it is not a useful feature\n",
    "    X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "    \n",
    "    # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "    # window_size = size of the sliding window in units\n",
    "    # nb_channels = number of feature channels\n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    \n",
    "    # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "    # pass it the config object\n",
    "    net = DeepConvLSTM(config=config)\n",
    "\n",
    "    # defines the loss and optimizer\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    # convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "    X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "    X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "    \n",
    "    # feed the datasets into the train function; can be imported from model.train\n",
    "    cross_participant_net, _, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, optimizer=opt, loss=loss, config=config, log_date=log_date, log_timestamp=log_timestamp)\n",
    "    \n",
    "    # the next bit prints out the average results per subject if you did everything correctly\n",
    "    cls = np.array(range(config['nb_classes']))\n",
    "    \n",
    "    print('\\nVALIDATION RESULTS FOR SUBJECT {0}: '.format(int(sbj) + 1))\n",
    "    print(\"\\nAvg. Accuracy: {0}\".format(jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. Precision: {0}\".format(precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. Recall: {0}\".format(recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. F1: {0}\".format(f1_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "\n",
    "    print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "    print(\"\\nAccuracy:\")\n",
    "    for i, rslt in enumerate(jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nPrecision:\")\n",
    "    for i, rslt in enumerate(precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nRecall:\")\n",
    "    for i, rslt in enumerate(recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nF1:\")\n",
    "    for i, rslt in enumerate(f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "\n",
    "    print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "    print(\"\\nTrain-Val-Accuracy Difference: {0}\".format(jaccard_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                      jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-Precision Difference: {0}\".format(precision_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                       precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-Recall Difference: {0}\".format(recall_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                    recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-F1 Difference: {0}\".format(f1_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                f1_score(val_output[:, 1], val_output[:, 0], average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTFNUytOl9BM"
   },
   "source": [
    "## 5.4 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXvP175Tl9BM"
   },
   "source": [
    "Now, after having implemented each of the validation techniques we want to get an unbiased view of how our trained algorithm perfoms on unseen data. To do so we use the testing set which we split off the original dataset within the first step of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4cmr2Tll9BM"
   },
   "source": [
    "### Task 5: Testing your trained networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHRkzAaul9BM"
   },
   "source": [
    "1. Apply the `apply_sliding_window()` function on top of the `test` data. (`lines 7-9`)\n",
    "2. (*Optional*) Omit the first feature column (subject_identifier) from the test dataset. (`lines 12-14`)\n",
    "3. Convert the feature columns of the test dataset to `float32` and label column to `uint8` for GPU compatibility. Use the [built-in function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) of a pandas dataframe called `astype()`. (`lines 17-18`)\n",
    "4. Using the `predict()` function of the DL-ARC GitHub to obtain results on the `test` data using each of the trained networks as input. The function is already imported for you. (`lines 20-29`)\n",
    "5. Which model does perform the best and why? Was this expected? Can you make out a reason why that is? \n",
    "6. What would you change about the pipeline we just created if your goal was to get the best predictions possible? Hint: think about the amount of data which actually trained your model in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9s6kwjul9BM"
   },
   "outputs": [],
   "source": [
    "from model.train import predict\n",
    "\n",
    "\n",
    "# in order to get reproducible results, we need to seed torch and other random parts of our implementation\n",
    "seed_torch(config['seed'])\n",
    "\n",
    "# apply the sliding window on top of both the test data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_test, y_test = apply_sliding_window(test_data[:, :-1], test_data[:, -1], sliding_window_size=config['sw_length'], unit=config['sw_unit'], sampling_rate=config['sampling_rate'], sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the test dataset\n",
    "# you need to do it if you did so during training and validation!\n",
    "X_test = X_test[:, :, 1:]\n",
    "\n",
    "# convert the features of test to float32 and labels to uint8 for GPU compatibility \n",
    "X_test, y_test = X_test.astype(np.float32), y_test.astype(np.uint8)\n",
    "\n",
    "# the next lines will print out the test results for each of the trained networks\n",
    "print('COMPILED TEST RESULTS: ')\n",
    "print('\\nTest results (train-valid-split): ')\n",
    "predict(X_test, y_test, train_valid_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (k-fold): ')\n",
    "predict(X_test, y_test, kfold_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (cross-participant): ')\n",
    "predict(X_test, y_test, cross_participant_net, config, log_date, log_timestamp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "validation_and_testing_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
