{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCl0_WQzkm89"
   },
   "source": [
    "# 6. Some things to try as next research steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAMW_jXBYC5N"
   },
   "source": [
    "## 6.1. Important Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_jHD4jxksEJ"
   },
   "source": [
    "If you are accessing this tutorial via [Google Colab](https://colab.research.google.com/github/mariusbock/dl-for-har/blob/main/tutorial_notebooks/training.ipynb), first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
    "1. On Google Colab, go to `Help` -> `View in English` \n",
    "2. Change the default language of your browser to `English`.\n",
    "\n",
    "To also ease the communication when communicating errors, enable line numbers within the settings of Colab.\n",
    "\n",
    "1. On Google Colab, go to `Tools` -> `Settings` -> `Editor` -> `Show line numbers`\n",
    "\n",
    "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
    "\n",
    "- `Runtime` -> `Change runtime type` -> `Dropdown` -> `GPU` -> `Save`\n",
    "\n",
    "**Hint:** you can auto-complete code in Colab via `ctrl` + `spacebar`\n",
    "\n",
    "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our [GitHub repository](https://github.com/mariusbock/dl-for-har).\n",
    "\n",
    "To get started with this notebook, you need to first run the code cell below. Please set `use_colab` to be `True` if you are accessing this notebook via Colab. If not, please set it to `False`. This code cell will make sure that imports from our GitHub repository will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU7hQXifkr0n",
    "outputId": "8280385c-4866-4a5b-ccaa-8ae270bdd4db"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "use_colab = True\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if use_colab:\n",
    "    # move to content directory and remove directory for a clean start \n",
    "    %cd /content/         \n",
    "    %rm -rf dl-for-har\n",
    "    # clone package repository (will throw error if already cloned)\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/       \n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "    \n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "print(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz2hB3O3YJX6"
   },
   "source": [
    "## 6.2. Loading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to be adapted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRMk_5R_kcrJ",
    "outputId": "6ada39db-44d6-4461-8116-4cee67c37ee5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from data_processing.sliding_window import apply_sliding_window\n",
    "from data_processing.preprocess_data import load_dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# data loading (we are using a predefined method called load_dataset, which is part of the DL-ARC feature stack)\n",
    "X, y, num_classes, class_names, sampling_rate, has_null = load_dataset('rwhar_3sbjs', include_null=True)\n",
    "# since the method returns features and labels separatley, we need to concat them\n",
    "data = np.concatenate((X, y[:, None]), axis=1)\n",
    "\n",
    "# define the train data to be all data belonging to the first two subjects\n",
    "train_data = data[data[:, 0] <= 1]\n",
    "# define the validation data to be all data belonging to the third subject\n",
    "valid_data = data[data[:, 0] == 2]\n",
    "\n",
    "# settings for the sliding window (change them if you want to!)\n",
    "sw_length = 50\n",
    "sw_unit = 'units'\n",
    "sw_overlap = 50\n",
    "\n",
    "# apply a sliding window on top of both the train and validation data; you can use our predefined method\n",
    "X_train, y_train = apply_sliding_window(train_data[:, :-1], train_data[:, -1], sliding_window_size=sw_length, unit=sw_unit, sampling_rate=50, sliding_window_overlap=sw_overlap)\n",
    "X_valid, y_valid = apply_sliding_window(valid_data[:, :-1], valid_data[:, -1], sliding_window_size=sw_length, unit=sw_unit, sampling_rate=50, sliding_window_overlap=sw_overlap)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "\n",
    "# convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLOXAncvYiRV"
   },
   "source": [
    "## 6.3. Training with and without Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whlsFyrVYpiL"
   },
   "source": [
    "### 6.3.1. Define the Config Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to be done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svIhikLBn_13"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    #### TRY AND CHANGE THESE PARAMETERS ####\n",
    "    # sliding window settings\n",
    "    'sw_length': 50,\n",
    "    'sw_unit': 'units',\n",
    "    'sampling_rate': 50,\n",
    "    'sw_overlap': 30,\n",
    "    # network settings\n",
    "    'nb_conv_blocks': 2,\n",
    "    'conv_block_type': 'normal',\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 11,\n",
    "    'nb_units_lstm': 128,\n",
    "    'nb_layers_lstm': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    # training settings\n",
    "    'epochs': 30,\n",
    "    'batch_size': 100,\n",
    "    'loss': 'cross_entropy',\n",
    "    'weighted': True,\n",
    "    'weights_init': 'xavier_uniform',\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-6,\n",
    "    ### UP FROM HERE YOU SHOULD RATHER NOT CHANGE THESE ####\n",
    "    'batch_norm': False,\n",
    "    'dilation': 1,\n",
    "    'pooling': False,\n",
    "    'pool_type': 'max',\n",
    "    'pool_kernel_width': 2,\n",
    "    'reduce_layer': False,\n",
    "    'reduce_layer_output': 10,\n",
    "    'nb_classes': 8,\n",
    "    'seed': 1,\n",
    "    'gpu': 'cuda:0',\n",
    "    'verbose': False,\n",
    "    'print_freq': 10,\n",
    "    'save_gradient_plot': False,\n",
    "    'print_counts': False,\n",
    "    'adj_lr': False,\n",
    "    'adj_lr_patience': 5,\n",
    "    'early_stopping': False,\n",
    "    'es_patience': 5,\n",
    "    'save_test_preds': False\n",
    "}\n",
    "\n",
    "\n",
    "config['window_size'] = X_train.shape[1]\n",
    "config['nb_channels'] = X_train.shape[2]\n",
    "config['nb_classes'] = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_8AsJ_MY8xR"
   },
   "source": [
    "### 6.3.3. Without Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to be done here - this has be covered / discussed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IF_SUtmFrVHd",
    "outputId": "a0467bc4-b62e-4bba-d17a-528bb3b6ab7f"
   },
   "outputs": [],
   "source": [
    "from model.DeepConvLSTM import DeepConvLSTM\n",
    "import model.train as training\n",
    "from datetime import datetime\n",
    "import torch\n",
    "seed_torch(config['seed'])\n",
    "\n",
    "# initialize your DeepConvLSTM object \n",
    "crossEntropyNet = DeepConvLSTM(config)\n",
    "#network.fc.bias = torch.nn.Parameter(torch.zeros_like(network.fc.bias), requires_grad=False)\n",
    "\n",
    "# initialize the optimizer and loss\n",
    "optimizer = torch.optim.Adam(crossEntropyNet.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "criterion = torch.nn.CrossEntropyLoss # should be the same as LabelSmoothingLoss(0.)\n",
    "\n",
    "# time and date for logging\n",
    "now = datetime.now()\n",
    "todayAsString = now.strftime(\"%d/%m/%Y\")\n",
    "timeAsString = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "crossEntropyNet, trainPredictions, validationPredictions = \\\n",
    "   training.train(X_train, y_train, X_valid, y_valid, crossEntropyNet, optimizer, \\\n",
    "                  criterion, config, todayAsString, timeAsString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 The Label Smoothing Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        neglog_softmaxPrediction = -prediction.log_softmax(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            smoothedLabels = self.smoothing / (prediction.size(1) - 1)* torch.ones_like(prediction)\n",
    "            smoothedLabels.scatter_(1, target.data.unsqueeze(1), 1-self.smoothing)\n",
    "        return torch.mean(torch.sum(smoothedLabels * neglog_softmaxPrediction, dim=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.5 With Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement / try out HAR with different amounts of label smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#   Your code goes here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 MaxUp Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section refers to the second part - data augmentation and maxup training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 MaxUp as a Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a suggestion, the Maxup class could consist of two functionalities: A forward pass that gets x and y as an input, and returns additional data as well as additional labels (increasing the amount of data by a factor of 'ntrials' or 'm' in the presentation). The second function is the adapted_loss that knows the structure of the data such that a reshaping is possible in order to have the 'ntrials'-many examples along one dimension, in which a maximum can be computed.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation inspired by https://github.com/JonasGeiping/data-poisoning/\n",
    "# see forest / data / mixing_data_augmentations.py\n",
    "\n",
    "class Maxup(torch.nn.Module):\n",
    "    \"\"\"A meta-augmentation, returning the worst result from a range of augmentations.\n",
    "    As in the orignal paper, https://arxiv.org/abs/2002.09024,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, given_data_augmentation, ntrials=4):\n",
    "        \"\"\"Initialize with a given data augmentation module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.augment = given_data_augmentation\n",
    "        self.ntrials = ntrials\n",
    "        self.max_criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        additional_x, additional_labels = [], []\n",
    "        for trial in range(self.ntrials):\n",
    "            x_out, y_out = self.augment(x, y)\n",
    "            additional_x.append(x_out)\n",
    "            additional_labels.append(y_out)\n",
    "\n",
    "        additional_x = torch.cat(additional_x, dim=0)\n",
    "        additional_labels = torch.cat(additional_labels, dim=0)\n",
    "        \n",
    "        return additional_x, additional_labels\n",
    "\n",
    "\n",
    "    def maxup_loss(self, outputs, extra_labels):\n",
    "        \"\"\"Compute loss. Here the loss is computed as worst-case estimate over the trials.\"\"\"\n",
    "        batch_size = outputs.shape[0] // self.ntrials\n",
    "        correct_preds = (torch.argmax(outputs.data, dim=1) == extra_labels).sum().item() / self.ntrials\n",
    "        stacked_loss = self.max_criterion(outputs, extra_labels).view(batch_size, self.ntrials, -1)\n",
    "        loss = stacked_loss.max(dim=1)[0].mean()\n",
    "        \n",
    "        return loss, correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myNoiseAdditionAugmenter(x,y):\n",
    "    #\n",
    "    # Todo: Write an augmentation that adds noise to x and keeps the label unchanged.\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "# Todo if you like: Possibly write more sophisticated data augmentation methods, e.g. inspired by Mixup, Cutout, or Cutmix.\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.2 Maxup in the training proceedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.DeepConvLSTM import DeepConvLSTM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize your DeepConvLSTM object \n",
    "seed_torch(config['seed'])\n",
    "network = DeepConvLSTM(config)\n",
    "\n",
    "# sends network to the GPU and sets it to training mode\n",
    "network.to(config['gpu'])\n",
    "network.train()\n",
    "\n",
    "# initialize the optimizer and loss\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "maxup = Maxup(myNoiseAdditionAugmenter, ntrials=4)\n",
    "\n",
    "\n",
    "# initialize training and validation dataset, define DataLoaders\n",
    "dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid))\n",
    "valloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "trainloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "\n",
    "# define your training loop; iterates over the number of epochs\n",
    "for e in range(config['epochs']):\n",
    "    counter = 1\n",
    "\n",
    "    # iterate over the trainloader object (it'll return batches which you can use)\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        # sends batch x and y to the GPU\n",
    "        inputs, targets = x.to(config['gpu']), y.to(config['gpu'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Increase the inputs via data augmentation\n",
    "        inputs,targets = maxup(inputs,targets)\n",
    "        \n",
    "        # send inputs through network to get predictions\n",
    "        train_output = network(inputs)\n",
    "        \n",
    "\n",
    "        # calculates loss\n",
    "        loss = maxup.maxup_loss(train_output, targets.long())[0]\n",
    "\n",
    "        # backprogate your computed loss through the network\n",
    "        # use the .backward() and .step() function on your loss and optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # prints out every 100 batches information about the current loss and time per batch\n",
    "        if counter % 80 == 0:\n",
    "            print('| epoch {:3d} | current minibatch train loss {:5.2f}'.format(e, loss.item()))\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun final accuracy comparison (has extreme variance based on the seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# Todo: evaluate / compare the networks you have trained.\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "next_research_steps_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}